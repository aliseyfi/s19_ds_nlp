{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction: The purpose of this homework will be to examine the similarity of a number of articles contained in the data directory. Specifically, the intent is to implement different document similarity techniques and then see how similar these documents are using said techniques. Finally, we would like to present the similarity in a table or heatmap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:\n",
    "In order to begin this exercise, we will first need to iterate over the data directory and read the content of each file. We will store the data from each file in a hash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "path_to_homework_2_data_directory = \"/Users/teacher/repos/s19_ds_nlp/homework_solutions/homework_2/data\"\n",
    "\n",
    "article_hash = {} # this hash should serve to represent the content of the files in the data directory\n",
    "# use the filename as the hash key and the value will be the text of the file\n",
    "# thus you would be able to retrieve an individual documents text like: article_hash[\"article_1\"]\n",
    "\n",
    "# here we will get a list of the filenames of things contained in the data directory\n",
    "files = [f for f in listdir(path_to_homework_2_data_directory) if isfile(join(path_to_homework_2_data_directory, f))]\n",
    "\n",
    "# here you will iterate over all the files contained in the directory\n",
    "for file in files:\n",
    "    file_location = join(path_to_homework_2_data_directory, file)\n",
    "    file_text = open(file_location, 'r')\n",
    "    article_hash[file] = file_text.read()\n",
    "    # article_hash[filename_variable] = the content of the file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2:\n",
    "Text processing. Now that you have the content of the files read into a hash you will be able to process them. Specifically, you should perhaps employ sentence segmentation, tokenization, and stemming to get a new representation of the document. Here you will want to build a sufficiently flexible approach so that you can try out several different pre-processing strategies to see how it affects your similarity scores.\n",
    "\n",
    "We'll create a new hash that contains the processed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# setup a new hash to store the results in\n",
    "processed_article_hash = {}\n",
    "\n",
    "# iterate through the keys, i.e. document ids, in the hash to pull out the stored text and process\n",
    "for key in article_hash.keys():\n",
    "    text_of_article = article_hash[key]\n",
    "    tokens = nlp(text_of_article)\n",
    "    token_lemmas = [token.lemma_ for token in tokens if token.is_alpha]\n",
    "    \n",
    "    \n",
    "    processed_article_hash[key] = token_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3:\n",
    "Implement two similarity techniques. \n",
    "We would like to examine the Jacard Similarity and Cosine.\n",
    "\n",
    "Jacardian Similarity: here we want to identify the set of words in two documents that overlap and then divide that by the count of unique words across both documents.\n",
    "\n",
    "Cosine Similarity: Here we want to create vector representations for each document. Specifically, we want to come up with a vector that is based on the list of all words that occur across both documents. Then for each document we will create a vector that includes the counts of the number of time a word occurs in the document.\n",
    "\n",
    "So if the document 1 is: \"the ship sails at midnight\" and document 2 is: \"the crow flies at noon.\" We would be creating a vector like: [the, ship, sails, at, midnight, crow, flies, noon]. Then we would calculate the values of the vector for each document. For document 1: [1,1,1,1,1,0,0,0] and for document 2: [1,0,0,1,0,1,1,1]. With these two vectors we would simply take the dot product and that would provide the cosine similarity. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacardian_distance(document_1_data, document_2_data):\n",
    "\n",
    "    words_in_doc_1_not_in_doc_2 = set(document_1_data) - set(document_2_data)\n",
    "    words_in_doc_2_not_in_doc_1 = set(document_2_data) - set(document_1_data)\n",
    "    words_in_both_doc_1_and_doc_2 = set(document_1_data).union(set(document_2_data))\n",
    "    \n",
    "    jacardian = len(set(document_1_data).intersection(document_2_data))/(len(words_in_both_doc_1_and_doc_2))\n",
    "    \n",
    "    return jacardian\n",
    "\n",
    "import numpy as np\n",
    "def cosine_similarity(document_1_data, document_2_data):\n",
    "\n",
    "    document_vector_word_index =     sorted(list(set(document_1_data).union(set(document_2_data))))\n",
    "    document_1_vector = [document_1_data.count(word) for word in document_vector_word_index] # fill in the array with the frequency of the words in the document\n",
    "    document_2_vector = [document_2_data.count(word) for word in document_vector_word_index] # fill in the array with the frequency of the words in the document\n",
    "    \n",
    "    return np.dot(document_1_vector, document_2_vector)/(np.linalg.norm(document_1_vector) * np.linalg.norm(document_2_vector)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4:\n",
    "Now that we have our two similarity measures, we want to examine each document relative to each other and calculate their similarity. \n",
    "\n",
    "So we will want to create two tables that show the document similarities using both techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALUES FOR:\n",
      "article_10\n",
      "{'article_10': 1.0, 'article_11': 0.7619047619047619, 'article_3': 0.6944444444444444, 'article_4': 0.7808219178082192, 'article_5': 0.7361111111111112, 'article_2': 0.72, 'article_12': 0.7837837837837838, 'article_9': 0.75, 'article_7': 0.7837837837837838, 'article_1': 0.6883116883116883, 'article_6': 0.7777777777777778, 'article_8': 0.7536231884057971}\n",
      "{'article_10': 1.0000000000000002, 'article_11': 0.9947009765583739, 'article_3': 0.9912794528428056, 'article_4': 0.988660431028577, 'article_5': 0.992271240801478, 'article_2': 0.9927117012038547, 'article_12': 0.9935793787694568, 'article_9': 0.9893410648010433, 'article_7': 0.9903740593563779, 'article_1': 0.9929176451935869, 'article_6': 0.9935983167439728, 'article_8': 0.9935447426962715}\n",
      "VALUES FOR:\n",
      "article_11\n",
      "{'article_10': 0.7619047619047619, 'article_11': 1.0, 'article_3': 0.746268656716418, 'article_4': 0.6233766233766234, 'article_5': 0.6438356164383562, 'article_2': 0.6986301369863014, 'article_12': 0.6282051282051282, 'article_9': 0.620253164556962, 'article_7': 0.6710526315789473, 'article_1': 0.6891891891891891, 'article_6': 0.618421052631579, 'article_8': 0.7575757575757576}\n",
      "{'article_10': 0.9947009765583739, 'article_11': 0.9999999999999999, 'article_3': 0.9826681403337115, 'article_4': 0.9774883830510533, 'article_5': 0.9834703864594622, 'article_2': 0.9868200871174101, 'article_12': 0.9873900913719382, 'article_9': 0.9809298978202459, 'article_7': 0.9805771152136686, 'article_1': 0.9912307901763264, 'article_6': 0.9871016197870338, 'article_8': 0.9923552241827005}\n",
      "VALUES FOR:\n",
      "article_3\n",
      "{'article_10': 0.6944444444444444, 'article_11': 0.746268656716418, 'article_3': 1.0, 'article_4': 0.8133333333333334, 'article_5': 0.7466666666666667, 'article_2': 0.9014084507042254, 'article_12': 0.7692307692307693, 'article_9': 0.782051282051282, 'article_7': 0.7468354430379747, 'article_1': 0.8888888888888888, 'article_6': 0.7866666666666666, 'article_8': 0.8405797101449275}\n",
      "{'article_10': 0.9912794528428056, 'article_11': 0.9826681403337115, 'article_3': 1.0000000000000002, 'article_4': 0.992916941890487, 'article_5': 0.9971529638781846, 'article_2': 0.9975348846413614, 'article_12': 0.9968617208740929, 'article_9': 0.9949722653931076, 'article_7': 0.9943009090745611, 'article_1': 0.9915933873309832, 'article_6': 0.994483267124391, 'article_8': 0.9925086043583342}\n",
      "VALUES FOR:\n",
      "article_4\n",
      "{'article_10': 0.7808219178082192, 'article_11': 0.6233766233766234, 'article_3': 0.8133333333333334, 'article_4': 1.0, 'article_5': 0.8289473684210527, 'article_2': 0.8571428571428571, 'article_12': 0.9210526315789473, 'article_9': 0.9090909090909091, 'article_7': 0.8481012658227848, 'article_1': 0.8227848101265823, 'article_6': 0.8933333333333333, 'article_8': 0.8}\n",
      "{'article_10': 0.988660431028577, 'article_11': 0.9774883830510533, 'article_3': 0.992916941890487, 'article_4': 1.0, 'article_5': 0.9897574451723112, 'article_2': 0.9930109070090312, 'article_12': 0.9900932888045327, 'article_9': 0.9911272489651987, 'article_7': 0.9970127560989821, 'article_1': 0.9853422036899474, 'article_6': 0.9895692755642238, 'article_8': 0.9890534401109905}\n",
      "VALUES FOR:\n",
      "article_5\n",
      "{'article_10': 0.7361111111111112, 'article_11': 0.6438356164383562, 'article_3': 0.7466666666666667, 'article_4': 0.8289473684210527, 'article_5': 1.0, 'article_2': 0.7692307692307693, 'article_12': 0.8076923076923077, 'article_9': 0.8441558441558441, 'article_7': 0.7625, 'article_1': 0.759493670886076, 'article_6': 0.8767123287671232, 'article_8': 0.7333333333333333}\n",
      "{'article_10': 0.992271240801478, 'article_11': 0.9834703864594622, 'article_3': 0.9971529638781846, 'article_4': 0.9897574451723112, 'article_5': 1.0, 'article_2': 0.9947466970358498, 'article_12': 0.9960602360901856, 'article_9': 0.992751239796189, 'article_7': 0.9917484452203994, 'article_1': 0.989557297164512, 'article_6': 0.9941850295753912, 'article_8': 0.9923548123197855}\n",
      "VALUES FOR:\n",
      "article_2\n",
      "{'article_10': 0.72, 'article_11': 0.6986301369863014, 'article_3': 0.9014084507042254, 'article_4': 0.8571428571428571, 'article_5': 0.7692307692307693, 'article_2': 1.0, 'article_12': 0.8589743589743589, 'article_9': 0.8481012658227848, 'article_7': 0.7901234567901234, 'article_1': 0.9324324324324325, 'article_6': 0.8311688311688312, 'article_8': 0.8873239436619719}\n",
      "{'article_10': 0.9927117012038547, 'article_11': 0.9868200871174101, 'article_3': 0.9975348846413614, 'article_4': 0.9930109070090312, 'article_5': 0.9947466970358498, 'article_2': 1.0000000000000002, 'article_12': 0.9965855417452526, 'article_9': 0.9951635006310252, 'article_7': 0.9946215802718598, 'article_1': 0.9942605501712543, 'article_6': 0.9932365522064658, 'article_8': 0.9937067630492805}\n",
      "VALUES FOR:\n",
      "article_12\n",
      "{'article_10': 0.7837837837837838, 'article_11': 0.6282051282051282, 'article_3': 0.7692307692307693, 'article_4': 0.9210526315789473, 'article_5': 0.8076923076923077, 'article_2': 0.8589743589743589, 'article_12': 1.0, 'article_9': 0.8860759493670886, 'article_7': 0.8734177215189873, 'article_1': 0.8024691358024691, 'article_6': 0.8701298701298701, 'article_8': 0.8026315789473685}\n",
      "{'article_10': 0.9935793787694568, 'article_11': 0.9873900913719382, 'article_3': 0.9968617208740929, 'article_4': 0.9900932888045327, 'article_5': 0.9960602360901856, 'article_2': 0.9965855417452526, 'article_12': 1.0, 'article_9': 0.9959228111293005, 'article_7': 0.9932604850935337, 'article_1': 0.9928456263650677, 'article_6': 0.9933762546077981, 'article_8': 0.994144545178497}\n",
      "VALUES FOR:\n",
      "article_9\n",
      "{'article_10': 0.75, 'article_11': 0.620253164556962, 'article_3': 0.782051282051282, 'article_4': 0.9090909090909091, 'article_5': 0.8441558441558441, 'article_2': 0.8481012658227848, 'article_12': 0.8860759493670886, 'article_9': 1.0, 'article_7': 0.8170731707317073, 'article_1': 0.8148148148148148, 'article_6': 0.8589743589743589, 'article_8': 0.7692307692307693}\n",
      "{'article_10': 0.9893410648010433, 'article_11': 0.9809298978202459, 'article_3': 0.9949722653931076, 'article_4': 0.9911272489651987, 'article_5': 0.992751239796189, 'article_2': 0.9951635006310252, 'article_12': 0.9959228111293005, 'article_9': 1.0000000000000002, 'article_7': 0.990961087308529, 'article_1': 0.9910872023321639, 'article_6': 0.9880820743141592, 'article_8': 0.9914400522346176}\n",
      "VALUES FOR:\n",
      "article_7\n",
      "{'article_10': 0.7837837837837838, 'article_11': 0.6710526315789473, 'article_3': 0.7468354430379747, 'article_4': 0.8481012658227848, 'article_5': 0.7625, 'article_2': 0.7901234567901234, 'article_12': 0.8734177215189873, 'article_9': 0.8170731707317073, 'article_7': 1.0, 'article_1': 0.8024691358024691, 'article_6': 0.8227848101265823, 'article_8': 0.7564102564102564}\n",
      "{'article_10': 0.9903740593563779, 'article_11': 0.9805771152136686, 'article_3': 0.9943009090745611, 'article_4': 0.9970127560989821, 'article_5': 0.9917484452203994, 'article_2': 0.9946215802718598, 'article_12': 0.9932604850935337, 'article_9': 0.990961087308529, 'article_7': 1.0, 'article_1': 0.9845670370800106, 'article_6': 0.9914804254168295, 'article_8': 0.9910261082348826}\n",
      "VALUES FOR:\n",
      "article_1\n",
      "{'article_10': 0.6883116883116883, 'article_11': 0.6891891891891891, 'article_3': 0.8888888888888888, 'article_4': 0.8227848101265823, 'article_5': 0.759493670886076, 'article_2': 0.9324324324324325, 'article_12': 0.8024691358024691, 'article_9': 0.8148148148148148, 'article_7': 0.8024691358024691, 'article_1': 1.0, 'article_6': 0.7974683544303798, 'article_8': 0.8493150684931506}\n",
      "{'article_10': 0.9929176451935869, 'article_11': 0.9912307901763264, 'article_3': 0.9915933873309832, 'article_4': 0.9853422036899474, 'article_5': 0.989557297164512, 'article_2': 0.9942605501712543, 'article_12': 0.9928456263650677, 'article_9': 0.9910872023321639, 'article_7': 0.9845670370800106, 'article_1': 1.0, 'article_6': 0.9914611467390263, 'article_8': 0.9918193023658004}\n",
      "VALUES FOR:\n",
      "article_6\n",
      "{'article_10': 0.7777777777777778, 'article_11': 0.618421052631579, 'article_3': 0.7866666666666666, 'article_4': 0.8933333333333333, 'article_5': 0.8767123287671232, 'article_2': 0.8311688311688312, 'article_12': 0.8701298701298701, 'article_9': 0.8589743589743589, 'article_7': 0.8227848101265823, 'article_1': 0.7974683544303798, 'article_6': 1.0, 'article_8': 0.7733333333333333}\n",
      "{'article_10': 0.9935983167439728, 'article_11': 0.9871016197870338, 'article_3': 0.994483267124391, 'article_4': 0.9895692755642238, 'article_5': 0.9941850295753912, 'article_2': 0.9932365522064658, 'article_12': 0.9933762546077981, 'article_9': 0.9880820743141592, 'article_7': 0.9914804254168295, 'article_1': 0.9914611467390263, 'article_6': 1.0, 'article_8': 0.9917674037479985}\n",
      "VALUES FOR:\n",
      "article_8\n",
      "{'article_10': 0.7536231884057971, 'article_11': 0.7575757575757576, 'article_3': 0.8405797101449275, 'article_4': 0.8, 'article_5': 0.7333333333333333, 'article_2': 0.8873239436619719, 'article_12': 0.8026315789473685, 'article_9': 0.7692307692307693, 'article_7': 0.7564102564102564, 'article_1': 0.8493150684931506, 'article_6': 0.7733333333333333, 'article_8': 1.0}\n",
      "{'article_10': 0.9935447426962715, 'article_11': 0.9923552241827005, 'article_3': 0.9925086043583342, 'article_4': 0.9890534401109905, 'article_5': 0.9923548123197855, 'article_2': 0.9937067630492805, 'article_12': 0.994144545178497, 'article_9': 0.9914400522346176, 'article_7': 0.9910261082348826, 'article_1': 0.9918193023658004, 'article_6': 0.9917674037479985, 'article_8': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# create a variable to store your table data... you could use a hash or some other data structure. \n",
    "# We just want it to identify which document is being compared to which other document.\n",
    "\n",
    "data_structure_for_jacard_similarity = {}\n",
    "data_structure_for_cosine_similarity = {}\n",
    "\n",
    "for doc_1_key in article_hash.keys():\n",
    "    for doc_2_key in article_hash.keys():\n",
    "        if doc_1_key not in data_structure_for_jacard_similarity.keys():\n",
    "            data_structure_for_jacard_similarity[doc_1_key] = {}\n",
    "        if doc_1_key not in data_structure_for_cosine_similarity.keys():\n",
    "            data_structure_for_cosine_similarity[doc_1_key] = {}\n",
    "\n",
    "\n",
    "        # we have the nested for loops as one way to compare each document to each other document\n",
    "        doc_1_processed_text = article_hash[doc_1_key]\n",
    "        doc_2_processed_text = article_hash[doc_2_key]\n",
    "        data_structure_for_jacard_similarity[doc_1_key][doc_2_key] = jacardian_distance(doc_1_processed_text, doc_2_processed_text)\n",
    "        data_structure_for_cosine_similarity[doc_1_key][doc_2_key] = cosine_similarity(doc_1_processed_text, doc_2_processed_text)\n",
    "\n",
    "\n",
    "        \n",
    "# finally, find some way to present this data back. Either as a straight table or a heatmap.\n",
    "for doc_key in article_hash.keys():\n",
    "    print(\"VALUES FOR:\")\n",
    "    print(doc_key)\n",
    "    print(data_structure_for_jacard_similarity[doc_key])\n",
    "    print(data_structure_for_cosine_similarity[doc_key])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5:\n",
    "You should now have two different similarity mechanisms. What do your results suggest? From perusing the documents, do you think the suggested ones are similar or not? Does tokenization, stemming, stop word removal or anything else improve your results?\n",
    "\n",
    "Write a brief description of your reactions to identifying these similar documents and what measures and pre-processing steps you think worked best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: In the above similarity measures, you have only put tokens into the computed similarity. What if you added named entities? \n",
    "\n",
    "For the bonus try adding named entities as distinct features into the similarity calculation. Here you could use Spacy and just get the entities off each document. One thing to keep in mind is that entities are chunks of text and not tokens, so you will want to put the whole entity text in as a separate feature for similarity. Another thing to consider is that since there are multiple types of entities you might also include the entity type along with the entity when you do your similarity comparisons (you could just concatenate the entity type and the entity text as a single token into your similarity comparisons).\n",
    "\n",
    "While this bonus question might seem tricky, it actually will allow for very unsophisticated implementations. I'll aim to discuss a similar technique in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<-- put your comments here -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
